{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Week 3 Hands-on Lab: Visualizing the Self-Attention Mechanism**"
      ],
      "metadata": {
        "id": "UoAdO2irAQX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this hands-on lab, we will visualize the self-attention mechanism in Large Language Models. This will help you understand how it works.\n",
        "\n",
        "This hands-on lab will allow you to:\n",
        "1.\tImplement and visualize the self-attention mechanism using Python.\n",
        "2.\tUnderstand how attention scores are calculated and applied to input data.\n",
        "3.\tGain hands-on experience in a simplified environment to appreciate the role of attention in NLP models.\n"
      ],
      "metadata": {
        "id": "AMArc39FAgY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Compute Self-Attention Scores**"
      ],
      "metadata": {
        "id": "kDwMj2v1BlvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.\tDefine Input Data**\n",
        "\n",
        "Create a toy dataset representing embeddings for a sentence."
      ],
      "metadata": {
        "id": "9uzeq7VSBrTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example sentence: \"The cat sat on the mat\"\n",
        "sentence = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "\n",
        "# Dummy embeddings for each word (6 words, 4-dimensional embeddings)\n",
        "embeddings = np.random.rand(6, 4)\n",
        "print(\"Word Embeddings:\\n\", embeddings)"
      ],
      "metadata": {
        "id": "-N6T5C9EBviH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.\tCreate Query, Key, and Value Matrices**\n",
        "\n",
        "Generate query, key, and value vectors for each word.\n"
      ],
      "metadata": {
        "id": "_DlaHi7_B2w_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_qkv(embeddings, d_k=4):\n",
        "    np.random.seed(0)  # For reproducibility\n",
        "    W_q = np.random.rand(d_k, d_k)  # Query weights\n",
        "    W_k = np.random.rand(d_k, d_k)  # Key weights\n",
        "    W_v = np.random.rand(d_k, d_k)  # Value weights\n",
        "    Q = np.dot(embeddings, W_q)\n",
        "    K = np.dot(embeddings, W_k)\n",
        "    V = np.dot(embeddings, W_v)\n",
        "    return Q, K, V\n",
        "\n",
        "Q, K, V = generate_qkv(embeddings)\n",
        "print(\"Query Vectors:\\n\", Q)\n",
        "print(\"Key Vectors:\\n\", K)\n",
        "print(\"Value Vectors:\\n\", V)"
      ],
      "metadata": {
        "id": "Ly4w43ruB9DI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.\tCompute Attention Scores**\n",
        "\n",
        "Calculate attention scores using the dot product of queries and keys, followed by a softmax function.\n"
      ],
      "metadata": {
        "id": "XVYtBMBMCA9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention_scores(Q, K):\n",
        "    d_k = Q.shape[1]  # Dimension of key vectors\n",
        "    scores = np.dot(Q, K.T) / np.sqrt(d_k)  # Scaled dot product\n",
        "    softmax_scores = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n",
        "    return softmax_scores\n",
        "\n",
        "attention = attention_scores(Q, K)\n",
        "print(\"Attention Scores:\\n\", attention)"
      ],
      "metadata": {
        "id": "NH0n4boZCFjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.\tApply Attention Scores to Value Vectors**\n",
        "\n",
        "Multiply attention scores with the value vectors to compute the final outputs.\n"
      ],
      "metadata": {
        "id": "6qpVtAN-CJIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention_output(attention, V):\n",
        "    return np.dot(attention, V)\n",
        "\n",
        "output = attention_output(attention, V)\n",
        "print(\"Self-Attention Output:\\n\", output)"
      ],
      "metadata": {
        "id": "nG0LEhJdCOIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: Visualize Attention**"
      ],
      "metadata": {
        "id": "6iLfHRCoCS2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\t**Create a Heatmap of Attention Scores**\n",
        "\n",
        "Use matplotlib to visualize the attention scores between words.\n"
      ],
      "metadata": {
        "id": "3a_7sazACWFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_attention(sentence, attention):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(attention, cmap='viridis')\n",
        "    plt.colorbar()\n",
        "    plt.xticks(ticks=range(len(sentence)), labels=sentence, rotation=90)\n",
        "    plt.yticks(ticks=range(len(sentence)), labels=sentence)\n",
        "    plt.title(\"Self-Attention Scores\")\n",
        "    plt.show()\n",
        "\n",
        "plot_attention(sentence, attention)"
      ],
      "metadata": {
        "id": "hp0ooiTyCwn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3: Experimentation**"
      ],
      "metadata": {
        "id": "8g6B7hUFCz_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\t**Change Sentence Length and Dimensions**\n",
        "\n",
        "Modify the sentence and embeddings variables to explore how attention adapts to different inputs.\n",
        "\n",
        "2.\t**Analyze Different Weight Matrices**\n",
        "\n",
        "Experiment with different random weights for query, key, and value matrices to observe changes in attention scores.\n",
        "\n",
        "3.\t**Add Masking**\n",
        "\n",
        "Implement masking to ignore certain positions (e.g., padding tokens) in the sequence.\n"
      ],
      "metadata": {
        "id": "i4FSNPCbC3iJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_attention_scores(Q, K, mask=None):\n",
        "    d_k = Q.shape[1]\n",
        "    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores += (mask * -1e9)  # Add large negative value to masked positions\n",
        "    softmax_scores = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n",
        "    return softmax_scores\n",
        "\n",
        "# Example mask: ignore the last word\n",
        "mask = np.array([[0, 0, 0, 0, 0, -1]])\n",
        "masked_attention = masked_attention_scores(Q, K, mask)\n",
        "plot_attention(sentence, masked_attention)"
      ],
      "metadata": {
        "id": "DjulWDhuDA5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bonus Part 4: Try the Bertviz library to visualize attention scores in a real model**"
      ],
      "metadata": {
        "id": "qtMWCTCiKFUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, we will experiment with the Bertviz library.\n",
        "BertViz is an interactive tool for visualizing attention in Transformer language models such as BERT, GPT2, or T5. It can be run inside a Jupyter or Colab notebook through a simple Python API that supports most Huggingface models."
      ],
      "metadata": {
        "id": "_A1Zr93rKQzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install the library\n",
        "!pip install bertviz"
      ],
      "metadata": {
        "id": "IZg0k3hSIhAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First load a Huggingface model, either a pre-trained model as shown below, or your own fine-tuned model. Be sure to set output_attentions=True."
      ],
      "metadata": {
        "id": "JFGMQjgnLVVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel, utils\n",
        "utils.logging.set_verbosity_error()  # Suppress standard warnings\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)"
      ],
      "metadata": {
        "id": "guUXv47oLSK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then prepare inputs and compute attention:"
      ],
      "metadata": {
        "id": "3-R8g1mGLkEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer.encode(\"The cat sat on the mat\", return_tensors='pt')\n",
        "outputs = model(inputs)\n",
        "attention = outputs[-1]  # Output includes attention weights when output_attentions=True\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs[0])"
      ],
      "metadata": {
        "id": "LWYUpXa_LnIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, display the attention weights using the [head_view](https://github.com/jessevig/bertviz/blob/master/bertviz/head_view.py) or [model_view](https://github.com/jessevig/bertviz/blob/master/bertviz/model_view.py) functions:"
      ],
      "metadata": {
        "id": "_XNP40zcLpzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bertviz import head_view\n",
        "#The head view visualizes attention for one or more attention heads in the same layer\n",
        "head_view(attention, tokens)"
      ],
      "metadata": {
        "id": "ERY2Uc1vLtlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertviz import model_view\n",
        "#The model view shows a bird's-eye view of attention across all layers and heads.\n",
        "model_view(attention, tokens)"
      ],
      "metadata": {
        "id": "XCMt3pjeM6PR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}